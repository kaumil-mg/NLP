{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"Dear HR,My name is Kaumil, and I have 4 years of experience as a data analyst. \n",
    "After coming across the job opening for a data analyst at Cybercom on LinkedIn, I believe that I would be a great fit for this position based on the job description. You can also find my web resume here. \n",
    "I have attached my resume for your reference.\n",
    "Regards,\n",
    "Kaumil \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization\n",
    "## Sentences -> paragraph\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear HR,My name is Kaumil, and I have 4 years of experience as a data analyst. \n",
      "After coming across the job opening for a data analyst at Cybercom on LinkedIn, I believe that I would be a great fit for this position based on the job description. You can also find my web resume here. \n",
      "I have attached my resume for your reference.\n",
      "Regards,\n",
      "Kaumil \n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dear',\n",
       " 'HR',\n",
       " ',',\n",
       " 'My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Kaumil',\n",
       " ',',\n",
       " 'and',\n",
       " 'I',\n",
       " 'have',\n",
       " '4',\n",
       " 'years',\n",
       " 'of',\n",
       " 'experience',\n",
       " 'as',\n",
       " 'a',\n",
       " 'data',\n",
       " 'analyst',\n",
       " '.',\n",
       " 'After',\n",
       " 'coming',\n",
       " 'across',\n",
       " 'the',\n",
       " 'job',\n",
       " 'opening',\n",
       " 'for',\n",
       " 'a',\n",
       " 'data',\n",
       " 'analyst',\n",
       " 'at',\n",
       " 'Cybercom',\n",
       " 'on',\n",
       " 'LinkedIn',\n",
       " ',',\n",
       " 'I',\n",
       " 'believe',\n",
       " 'that',\n",
       " 'I',\n",
       " 'would',\n",
       " 'be',\n",
       " 'a',\n",
       " 'great',\n",
       " 'fit',\n",
       " 'for',\n",
       " 'this',\n",
       " 'position',\n",
       " 'based',\n",
       " 'on',\n",
       " 'the',\n",
       " 'job',\n",
       " 'description',\n",
       " '.',\n",
       " 'You',\n",
       " 'can',\n",
       " 'also',\n",
       " 'find',\n",
       " 'my',\n",
       " 'web',\n",
       " 'resume',\n",
       " 'here',\n",
       " '.',\n",
       " 'I',\n",
       " 'have',\n",
       " 'attached',\n",
       " 'my',\n",
       " 'resume',\n",
       " 'for',\n",
       " 'your',\n",
       " 'reference',\n",
       " '.',\n",
       " 'Regards',\n",
       " ',',\n",
       " 'Kaumil']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dear', 'HR', ',', 'My', 'name', 'is', 'Kaumil', ',', 'and', 'I', 'have', '4', 'years', 'of', 'experience', 'as', 'a', 'data', 'analyst', '.']\n",
      "['After', 'coming', 'across', 'the', 'job', 'opening', 'for', 'a', 'data', 'analyst', 'at', 'Cybercom', 'on', 'LinkedIn', ',', 'I', 'believe', 'that', 'I', 'would', 'be', 'a', 'great', 'fit', 'for', 'this', 'position', 'based', 'on', 'the', 'job', 'description', '.']\n",
      "['You', 'can', 'also', 'find', 'my', 'web', 'resume', 'here', '.']\n",
      "['I', 'have', 'attached', 'my', 'resume', 'for', 'your', 'reference', '.']\n",
      "['Regards', ',', 'Kaumil']\n"
     ]
    }
   ],
   "source": [
    "for sent in docs:\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dear',\n",
       " 'HR',\n",
       " ',',\n",
       " 'My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Kaumil',\n",
       " ',',\n",
       " 'and',\n",
       " 'I',\n",
       " 'have',\n",
       " '4',\n",
       " 'years',\n",
       " 'of',\n",
       " 'experience',\n",
       " 'as',\n",
       " 'a',\n",
       " 'data',\n",
       " 'analyst',\n",
       " '.',\n",
       " 'After',\n",
       " 'coming',\n",
       " 'across',\n",
       " 'the',\n",
       " 'job',\n",
       " 'opening',\n",
       " 'for',\n",
       " 'a',\n",
       " 'data',\n",
       " 'analyst',\n",
       " 'at',\n",
       " 'Cybercom',\n",
       " 'on',\n",
       " 'LinkedIn',\n",
       " ',',\n",
       " 'I',\n",
       " 'believe',\n",
       " 'that',\n",
       " 'I',\n",
       " 'would',\n",
       " 'be',\n",
       " 'a',\n",
       " 'great',\n",
       " 'fit',\n",
       " 'for',\n",
       " 'this',\n",
       " 'position',\n",
       " 'based',\n",
       " 'on',\n",
       " 'the',\n",
       " 'job',\n",
       " 'description',\n",
       " '.',\n",
       " 'You',\n",
       " 'can',\n",
       " 'also',\n",
       " 'find',\n",
       " 'my',\n",
       " 'web',\n",
       " 'resume',\n",
       " 'here',\n",
       " '.',\n",
       " 'I',\n",
       " 'have',\n",
       " 'attached',\n",
       " 'my',\n",
       " 'resume',\n",
       " 'for',\n",
       " 'your',\n",
       " 'reference',\n",
       " '.',\n",
       " 'Regards',\n",
       " ',',\n",
       " 'Kaumil']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "corpus=tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming \n",
    "words =  ['eating','eats','eaten','giving','gave','gives','writing','writes','programming','programs','history','histories','finally','finals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porter stemmer\n",
    "\n",
    "from nltk.stem import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming =PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear------>dear\n",
      "HR------>hr\n",
      ",------>,\n",
      "My------>my\n",
      "name------>name\n",
      "is------>is\n",
      "Kaumil------>kaumil\n",
      ",------>,\n",
      "and------>and\n",
      "I------>i\n",
      "have------>have\n",
      "4------>4\n",
      "years------>year\n",
      "of------>of\n",
      "experience------>experi\n",
      "as------>as\n",
      "a------>a\n",
      "data------>data\n",
      "analyst.------>analyst.\n",
      "After------>after\n",
      "coming------>come\n",
      "across------>across\n",
      "the------>the\n",
      "job------>job\n",
      "opening------>open\n",
      "for------>for\n",
      "a------>a\n",
      "data------>data\n",
      "analyst------>analyst\n",
      "at------>at\n",
      "Cybercom------>cybercom\n",
      "on------>on\n",
      "LinkedIn------>linkedin\n",
      ",------>,\n",
      "I------>i\n",
      "believe------>believ\n",
      "that------>that\n",
      "I------>i\n",
      "would------>would\n",
      "be------>be\n",
      "a------>a\n",
      "great------>great\n",
      "fit------>fit\n",
      "for------>for\n",
      "this------>thi\n",
      "position------>posit\n",
      "based------>base\n",
      "on------>on\n",
      "the------>the\n",
      "job------>job\n",
      "description.------>description.\n",
      "You------>you\n",
      "can------>can\n",
      "also------>also\n",
      "find------>find\n",
      "my------>my\n",
      "web------>web\n",
      "resume------>resum\n",
      "here.------>here.\n",
      "I------>i\n",
      "have------>have\n",
      "attached------>attach\n",
      "my------>my\n",
      "resume------>resum\n",
      "for------>for\n",
      "your------>your\n",
      "reference.------>reference.\n",
      "Regards------>regard\n",
      ",------>,\n",
      "Kaumil------>kaumil\n"
     ]
    }
   ],
   "source": [
    "for word in corpus:\n",
    "    print(word+\"------>\"+stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RegExStemmer class\n",
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating-------->eat\n",
      "eats-------->eat\n",
      "eaten-------->eat\n",
      "giving-------->giv\n",
      "gave-------->gave\n",
      "gives-------->give\n",
      "writing-------->writ\n",
      "writes-------->write\n",
      "programming-------->programm\n",
      "programs-------->program\n",
      "history-------->history\n",
      "histories-------->historie\n",
      "finally-------->finally\n",
      "finals-------->final\n"
     ]
    }
   ],
   "source": [
    "reg_stemmer =RegexpStemmer('ing$|s$|en$|able$', min=4)\n",
    "for i in words:\n",
    "    print(i+'-------->'+reg_stemmer.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowball stemmer\n",
    "\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating------>eat\n",
      "eats------>eat\n",
      "eaten------>eaten\n",
      "giving------>give\n",
      "gave------>gave\n",
      "gives------>give\n",
      "writing------>write\n",
      "writes------>write\n",
      "programming------>program\n",
      "programs------>program\n",
      "history------>histori\n",
      "histories------>histori\n",
      "finally------>final\n",
      "finals------>final\n"
     ]
    }
   ],
   "source": [
    "snowball = SnowballStemmer('english')\n",
    "for i in words:\n",
    "    print(i+'------>'+snowball.stem(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
