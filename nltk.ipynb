{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"Dear HR,My name is Kaumil, and I have 4 years of experience as a data analyst. \n",
    "After coming across the job opening for a data analyst at Cybercom on LinkedIn, I believe that I would be a great fit for this position based on the job description. You can also find my web resume here. \n",
    "I have attached my resume for your reference.\n",
    "Regards,\n",
    "Kaumil \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization\n",
    "## Sentences -> paragraph\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear HR,My name is Kaumil, and I have 4 years of experience as a data analyst. \n",
      "After coming across the job opening for a data analyst at Cybercom on LinkedIn, I believe that I would be a great fit for this position based on the job description. You can also find my web resume here. \n",
      "I have attached my resume for your reference.\n",
      "Regards,\n",
      "Kaumil \n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dear',\n",
       " 'HR',\n",
       " ',',\n",
       " 'My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Kaumil',\n",
       " ',',\n",
       " 'and',\n",
       " 'I',\n",
       " 'have',\n",
       " '4',\n",
       " 'years',\n",
       " 'of',\n",
       " 'experience',\n",
       " 'as',\n",
       " 'a',\n",
       " 'data',\n",
       " 'analyst',\n",
       " '.',\n",
       " 'After',\n",
       " 'coming',\n",
       " 'across',\n",
       " 'the',\n",
       " 'job',\n",
       " 'opening',\n",
       " 'for',\n",
       " 'a',\n",
       " 'data',\n",
       " 'analyst',\n",
       " 'at',\n",
       " 'Cybercom',\n",
       " 'on',\n",
       " 'LinkedIn',\n",
       " ',',\n",
       " 'I',\n",
       " 'believe',\n",
       " 'that',\n",
       " 'I',\n",
       " 'would',\n",
       " 'be',\n",
       " 'a',\n",
       " 'great',\n",
       " 'fit',\n",
       " 'for',\n",
       " 'this',\n",
       " 'position',\n",
       " 'based',\n",
       " 'on',\n",
       " 'the',\n",
       " 'job',\n",
       " 'description',\n",
       " '.',\n",
       " 'You',\n",
       " 'can',\n",
       " 'also',\n",
       " 'find',\n",
       " 'my',\n",
       " 'web',\n",
       " 'resume',\n",
       " 'here',\n",
       " '.',\n",
       " 'I',\n",
       " 'have',\n",
       " 'attached',\n",
       " 'my',\n",
       " 'resume',\n",
       " 'for',\n",
       " 'your',\n",
       " 'reference',\n",
       " '.',\n",
       " 'Regards',\n",
       " ',',\n",
       " 'Kaumil']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dear', 'HR', ',', 'My', 'name', 'is', 'Kaumil', ',', 'and', 'I', 'have', '4', 'years', 'of', 'experience', 'as', 'a', 'data', 'analyst', '.']\n",
      "['After', 'coming', 'across', 'the', 'job', 'opening', 'for', 'a', 'data', 'analyst', 'at', 'Cybercom', 'on', 'LinkedIn', ',', 'I', 'believe', 'that', 'I', 'would', 'be', 'a', 'great', 'fit', 'for', 'this', 'position', 'based', 'on', 'the', 'job', 'description', '.']\n",
      "['You', 'can', 'also', 'find', 'my', 'web', 'resume', 'here', '.']\n",
      "['I', 'have', 'attached', 'my', 'resume', 'for', 'your', 'reference', '.']\n",
      "['Regards', ',', 'Kaumil']\n"
     ]
    }
   ],
   "source": [
    "for sent in docs:\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dear',\n",
       " 'HR',\n",
       " ',',\n",
       " 'My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Kaumil',\n",
       " ',',\n",
       " 'and',\n",
       " 'I',\n",
       " 'have',\n",
       " '4',\n",
       " 'years',\n",
       " 'of',\n",
       " 'experience',\n",
       " 'as',\n",
       " 'a',\n",
       " 'data',\n",
       " 'analyst',\n",
       " '.',\n",
       " 'After',\n",
       " 'coming',\n",
       " 'across',\n",
       " 'the',\n",
       " 'job',\n",
       " 'opening',\n",
       " 'for',\n",
       " 'a',\n",
       " 'data',\n",
       " 'analyst',\n",
       " 'at',\n",
       " 'Cybercom',\n",
       " 'on',\n",
       " 'LinkedIn',\n",
       " ',',\n",
       " 'I',\n",
       " 'believe',\n",
       " 'that',\n",
       " 'I',\n",
       " 'would',\n",
       " 'be',\n",
       " 'a',\n",
       " 'great',\n",
       " 'fit',\n",
       " 'for',\n",
       " 'this',\n",
       " 'position',\n",
       " 'based',\n",
       " 'on',\n",
       " 'the',\n",
       " 'job',\n",
       " 'description',\n",
       " '.',\n",
       " 'You',\n",
       " 'can',\n",
       " 'also',\n",
       " 'find',\n",
       " 'my',\n",
       " 'web',\n",
       " 'resume',\n",
       " 'here',\n",
       " '.',\n",
       " 'I',\n",
       " 'have',\n",
       " 'attached',\n",
       " 'my',\n",
       " 'resume',\n",
       " 'for',\n",
       " 'your',\n",
       " 'reference',\n",
       " '.',\n",
       " 'Regards',\n",
       " ',',\n",
       " 'Kaumil']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "corpus=tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming \n",
    "words =  ['eating','eats','eaten','giving','gave','gives','writing','writes','programming','programs','history','histories','finally','finals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porter stemmer\n",
    "\n",
    "from nltk.stem import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming =PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear------>dear\n",
      "HR------>hr\n",
      ",------>,\n",
      "My------>my\n",
      "name------>name\n",
      "is------>is\n",
      "Kaumil------>kaumil\n",
      ",------>,\n",
      "and------>and\n",
      "I------>i\n",
      "have------>have\n",
      "4------>4\n",
      "years------>year\n",
      "of------>of\n",
      "experience------>experi\n",
      "as------>as\n",
      "a------>a\n",
      "data------>data\n",
      "analyst.------>analyst.\n",
      "After------>after\n",
      "coming------>come\n",
      "across------>across\n",
      "the------>the\n",
      "job------>job\n",
      "opening------>open\n",
      "for------>for\n",
      "a------>a\n",
      "data------>data\n",
      "analyst------>analyst\n",
      "at------>at\n",
      "Cybercom------>cybercom\n",
      "on------>on\n",
      "LinkedIn------>linkedin\n",
      ",------>,\n",
      "I------>i\n",
      "believe------>believ\n",
      "that------>that\n",
      "I------>i\n",
      "would------>would\n",
      "be------>be\n",
      "a------>a\n",
      "great------>great\n",
      "fit------>fit\n",
      "for------>for\n",
      "this------>thi\n",
      "position------>posit\n",
      "based------>base\n",
      "on------>on\n",
      "the------>the\n",
      "job------>job\n",
      "description.------>description.\n",
      "You------>you\n",
      "can------>can\n",
      "also------>also\n",
      "find------>find\n",
      "my------>my\n",
      "web------>web\n",
      "resume------>resum\n",
      "here.------>here.\n",
      "I------>i\n",
      "have------>have\n",
      "attached------>attach\n",
      "my------>my\n",
      "resume------>resum\n",
      "for------>for\n",
      "your------>your\n",
      "reference.------>reference.\n",
      "Regards------>regard\n",
      ",------>,\n",
      "Kaumil------>kaumil\n"
     ]
    }
   ],
   "source": [
    "for i in corpus:\n",
    "    print(i+\"------>\"+stemming.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RegExStemmer class\n",
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating-------->eat\n",
      "eats-------->eat\n",
      "eaten-------->eat\n",
      "giving-------->giv\n",
      "gave-------->gave\n",
      "gives-------->give\n",
      "writing-------->writ\n",
      "writes-------->write\n",
      "programming-------->programm\n",
      "programs-------->program\n",
      "history-------->history\n",
      "histories-------->historie\n",
      "finally-------->finally\n",
      "finals-------->final\n"
     ]
    }
   ],
   "source": [
    "reg_stemmer =RegexpStemmer('ing$|s$|en$|able$', min=4)\n",
    "for i in words:\n",
    "    print(i+'-------->'+reg_stemmer.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowball stemmer\n",
    "\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating------>eat\n",
      "eats------>eat\n",
      "eaten------>eaten\n",
      "giving------>give\n",
      "gave------>gave\n",
      "gives------>give\n",
      "writing------>write\n",
      "writes------>write\n",
      "programming------>program\n",
      "programs------>program\n",
      "history------>histori\n",
      "histories------>histori\n",
      "finally------>final\n",
      "finals------>final\n"
     ]
    }
   ],
   "source": [
    "snowball = SnowballStemmer('english')\n",
    "for i in words:\n",
    "    print(i+'------>'+snowball.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmetization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters\n",
    "word : str\n",
    "The input word to lemmatize.\n",
    "\n",
    "pos : str\n",
    "The Part Of Speech tag. Valid options are \"n\" for nouns, \"v\" for verbs, \"a\" for adjectives, \"r\" for adverbs and \"s\" for satellite adjectives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fly'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "wordnet.lemmatize('flying',pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating-------->eat\n",
      "eats-------->eat\n",
      "eaten-------->eat\n",
      "giving-------->give\n",
      "gave-------->give\n",
      "gives-------->give\n",
      "writing-------->write\n",
      "writes-------->write\n",
      "programming-------->program\n",
      "programs-------->program\n",
      "history-------->history\n",
      "histories-------->histories\n",
      "finally-------->finally\n",
      "finals-------->finals\n"
     ]
    }
   ],
   "source": [
    "for i in words:\n",
    "    print(i+'-------->'+wordnet.lemmatize(i,pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Stopwords\n",
    "speech =\"\"\"Good day, everyone,\n",
    "\n",
    "Today, I want to share some interesting findings from the analysis of cricket matches over the last three years and how this data can help us predict which teams might win in 2025.\n",
    "\n",
    "What We Learned from the Past Three Years\n",
    "Total Runs and Team Strength: Teams with strong batting lineups have consistently scored high runs. Teams like Sunrisers Hyderabad and Mumbai Indians have been particularly strong, often posting high scores, especially when they win the toss and choose to bat first.\n",
    "\n",
    "The Importance of the Toss: The toss has shown to play a big role in deciding the outcome of a match. Teams that win the toss and choose to bat first tend to perform better, setting higher scores and putting pressure on the other team.\n",
    "\n",
    "Impact of the Venue: The stadium where a match is played also makes a difference. Some grounds, like Eden Gardens and Wankhede Stadium, tend to favor teams with strong batting, leading to higher-scoring matches.\n",
    "\n",
    "What Can We Expect in 2025?\n",
    "With the help of machine learning, we've built a model that looks at all these factors—team strength, toss results, and match venues—to predict which teams are likely to win in 2025. Based on current trends, teams like Chennai Super Kings and Delhi Capitals look promising for the upcoming season.\n",
    "\n",
    "While the data can give us a good idea of what to expect, cricket is full of surprises, and that’s what makes the game so exciting.\n",
    "\n",
    "Thank you!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good day, everyone,\n",
      "\n",
      "Today, I want to share some interesting findings from the analysis of cricket matches over the last three years and how this data can help us predict which teams might win in 2025.\n",
      "\n",
      "What We Learned from the Past Three Years\n",
      "Total Runs and Team Strength: Teams with strong batting lineups have consistently scored high runs. Teams like Sunrisers Hyderabad and Mumbai Indians have been particularly strong, often posting high scores, especially when they win the toss and choose to bat first.\n",
      "\n",
      "The Importance of the Toss: The toss has shown to play a big role in deciding the outcome of a match. Teams that win the toss and choose to bat first tend to perform better, setting higher scores and putting pressure on the other team.\n",
      "\n",
      "Impact of the Venue: The stadium where a match is played also makes a difference. Some grounds, like Eden Gardens and Wankhede Stadium, tend to favor teams with strong batting, leading to higher-scoring matches.\n",
      "\n",
      "What Can We Expect in 2025?\n",
      "With the help of machine learning, we've built a model that looks at all these factors—team strength, toss results, and match venues—to predict which teams are likely to win in 2025. Based on current trends, teams like Chennai Super Kings and Delhi Capitals look promising for the upcoming season.\n",
      "\n",
      "While the data can give us a good idea of what to expect, cricket is full of surprises, and that’s what makes the game so exciting.\n",
      "\n",
      "Thank you!\n"
     ]
    }
   ],
   "source": [
    "print(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "word=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "sent=nltk.sent_tokenize(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good day, everyone,\\n\\nToday, I want to share some interesting findings from the analysis of cricket matches over the last three years and how this data can help us predict which teams might win in 2025.',\n",
       " 'What We Learned from the Past Three Years\\nTotal Runs and Team Strength: Teams with strong batting lineups have consistently scored high runs.',\n",
       " 'Teams like Sunrisers Hyderabad and Mumbai Indians have been particularly strong, often posting high scores, especially when they win the toss and choose to bat first.',\n",
       " 'The Importance of the Toss: The toss has shown to play a big role in deciding the outcome of a match.',\n",
       " 'Teams that win the toss and choose to bat first tend to perform better, setting higher scores and putting pressure on the other team.',\n",
       " 'Impact of the Venue: The stadium where a match is played also makes a difference.',\n",
       " 'Some grounds, like Eden Gardens and Wankhede Stadium, tend to favor teams with strong batting, leading to higher-scoring matches.',\n",
       " 'What Can We Expect in 2025?',\n",
       " \"With the help of machine learning, we've built a model that looks at all these factors—team strength, toss results, and match venues—to predict which teams are likely to win in 2025.\",\n",
       " 'Based on current trends, teams like Chennai Super Kings and Delhi Capitals look promising for the upcoming season.',\n",
       " 'While the data can give us a good idea of what to expect, cricket is full of surprises, and that’s what makes the game so exciting.',\n",
       " 'Thank you!']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sent)):\n\u001b[1;32m      4\u001b[0m     words1\u001b[38;5;241m=\u001b[39mnltk\u001b[38;5;241m.\u001b[39mword_tokenize(sent[j])\n\u001b[0;32m----> 5\u001b[0m     words1\u001b[38;5;241m=\u001b[39m[\u001b[43mstemmer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mj\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words1 \u001b[38;5;28;01mif\u001b[39;00m j \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))]\n\u001b[1;32m      6\u001b[0m     sent[j]\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(word)\n",
      "File \u001b[0;32m/workspaces/NLP/.venv/lib/python3.12/site-packages/nltk/stem/snowball.py:1409\u001b[0m, in \u001b[0;36mEnglishStemmer.stem\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstem\u001b[39m(\u001b[38;5;28mself\u001b[39m, word):\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;124;03m    Stem an English word and return the stemmed form.\u001b[39;00m\n\u001b[1;32m   1402\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1407\u001b[0m \n\u001b[1;32m   1408\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1409\u001b[0m     word \u001b[38;5;241m=\u001b[39m \u001b[43mword\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstopwords \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(word) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1412\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m word\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# Apply stopwords and filltering -> apply stemming\n",
    "\n",
    "for j in range(len(sent)):\n",
    "    words1=nltk.word_tokenize(sent[j])\n",
    "    words1=[stemmer.stem(j)for word in words1 if j not in set(stopwords.words('english'))]\n",
    "    sent[j]=' '.join(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
